{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset import Twibot20\n",
    "# import torch\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dev.json\n",
      "Loading train.json\n",
      "Loading test.json\n",
      "Loading support.json\n",
      "size of support.json: (217754, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Loading dev.json')\n",
    "df_dev=pd.read_json('./Twibot-20/dev.json')\n",
    "print('Loading train.json')\n",
    "df_train=pd.read_json('./Twibot-20/train.json')\n",
    "print('Loading test.json')\n",
    "df_test=pd.read_json('./Twibot-20/test.json')\n",
    "\n",
    "print('Loading support.json')\n",
    "df_support=pd.read_json('./Twibot-20/support.json')\n",
    "print('size of support.json:',df_support.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examineDataframes():\n",
    "    print('Examining Dataframes')\n",
    "    print('Columns:')\n",
    "    print(df_dev.columns)\n",
    "\n",
    "    print('\\n\\n')\n",
    "    print('Contents of tweet for index 1')\n",
    "    # print(df_dev.iloc[1]['tweet'])\n",
    "    # print(type(df_dev.iloc[1]['tweet']))\n",
    "    \n",
    "    print('Number of nodes in train.json:')\n",
    "    print(len(df_train))\n",
    "   \n",
    "    print('Number of nodes in test.json:')\n",
    "    print(len(df_test))\n",
    "\n",
    "    print('Number of nodes in dev.json:')\n",
    "    print(len(df_dev))\n",
    "    \n",
    "    \n",
    "    pass\n",
    "\n",
    "def langDetectDataset():\n",
    "    # print('Loading train.json')\n",
    "    # df_train=pd.read_json('./Twibot-20/train.json')\n",
    "    # print('Loading test.json')\n",
    "    # df_test=pd.read_json('./Twibot-20/test.json')\n",
    "    # print('Loading support.json')\n",
    "    # df_support=pd.read_json('./Twibot-20/support.json')\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "    nlp.add_pipe('language_detector', last=True)\n",
    "    \n",
    "    # df_combined = pd.concat([df_train, df_test, df_dev, df_support])\n",
    "    print('testing lang detection on test.json')\n",
    "    df_test['lang'] = df_test['tweet'].apply(lambda x: pd.Series(map(lambda y: nlp(y)._.language, x)) if x else None)\n",
    "    print(\"Language value count result for df_test:\")\n",
    "    print(df_test['lang'].value_counts())\n",
    "\n",
    "    # print('Language value count result for combined')\n",
    "    # print(df_combined['tweet'].apply(lambda x: pd.Series(x)).apply(lambda x: nlp(x)._.language).stack().value_counts())\n",
    "    # print(df_combined['lang'].value_counts())\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "def basicLangDetectExample():\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "    nlp.add_pipe('language_detector', last=True)\n",
    "    text = 'This is an english text.'\n",
    "    text2 = 'Ich bin ein deutscher Text.'\n",
    "    text3 = \"Je suis un texte en français.\"\n",
    "    doc = nlp(text)\n",
    "    doc2 = nlp(text2)\n",
    "    doc3 = nlp(text3)\n",
    "    # document level language detection. Think of it like average language of the document!\n",
    "    print(doc._.language)\n",
    "    # sentence level language detection\n",
    "    for sent in doc.sents:\n",
    "        print(sent, sent._.language)\n",
    "\n",
    "     # document level language detection. Think of it like average language of the document!\n",
    "    print(doc2._.language)\n",
    "    # sentence level language detection\n",
    "    for sent in doc2.sents:\n",
    "        print(sent, sent._.language)\n",
    "    \n",
    "     # document level language detection. Think of it like average language of the document!\n",
    "    print(doc3._.language)\n",
    "    # sentence level language detection\n",
    "    for sent in doc3.sents:\n",
    "        print(sent, sent._.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining Dataframes\n",
      "Columns:\n",
      "Index(['ID', 'profile', 'tweet', 'neighbor', 'domain', 'label'], dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "Contents of tweet for index 1\n",
      "Number of nodes in train.json:\n",
      "8278\n",
      "Number of nodes in test.json:\n",
      "1183\n",
      "Number of nodes in dev.json:\n",
      "2365\n"
     ]
    }
   ],
   "source": [
    "examineDataframes()\n",
    "# langDetectDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         ID  \\\n",
      "28304    986120930173726720   \n",
      "207641           1398479138   \n",
      "152589             26144649   \n",
      "118104   859525908826378240   \n",
      "123370            127214360   \n",
      "...                     ...   \n",
      "37375             246470557   \n",
      "27622             119051398   \n",
      "67006            2249436197   \n",
      "203749  1207685691695603712   \n",
      "111959             18265400   \n",
      "\n",
      "                                                  profile  \\\n",
      "28304   {'id': '986120930173726720 ', 'id_str': '98612...   \n",
      "207641  {'id': '1398479138 ', 'id_str': '1398479138 ',...   \n",
      "152589  {'id': '26144649 ', 'id_str': '26144649 ', 'na...   \n",
      "118104  {'id': '859525908826378250 ', 'id_str': '85952...   \n",
      "123370  {'id': '127214360 ', 'id_str': '127214360 ', '...   \n",
      "...                                                   ...   \n",
      "37375   {'id': '246470557 ', 'id_str': '246470557 ', '...   \n",
      "27622   {'id': '119051398 ', 'id_str': '119051398 ', '...   \n",
      "67006   {'id': '2249436197 ', 'id_str': '2249436197 ',...   \n",
      "203749  {'id': '1207685691695603718 ', 'id_str': '1207...   \n",
      "111959  {'id': '18265400 ', 'id_str': '18265400 ', 'na...   \n",
      "\n",
      "                                                    tweet neighbor  \\\n",
      "28304   [@dropshippin @fake_monitor @bkantha1 It’s my ...     None   \n",
      "207641  [RT @Corinne_Torres: The continuation of #melb...     None   \n",
      "152589  [RT @StephenKing: Trump was incompetent from t...     None   \n",
      "118104  [@32Atom23 Gracias!\\n, @ElWarrener Soy normal,...     None   \n",
      "123370  [RT @PartyInitiative: The truth! Meghan has be...     None   \n",
      "...                                                   ...      ...   \n",
      "37375   [I love this song. They need us. https://t.co/...     None   \n",
      "27622   [La banca digital es parte de la operación de ...     None   \n",
      "67006   [@ThomasErdbrink IRI  made hard for all of us ...     None   \n",
      "203749  [Você pode ganhar este EBOOK fazendo uma das 4...     None   \n",
      "111959  [RT @JoeBiden: Donald Trump keeps telling us i...     None   \n",
      "\n",
      "                                   domain  \n",
      "28304                          [Politics]  \n",
      "207641                           [Sports]  \n",
      "152589                    [Entertainment]  \n",
      "118104                    [Entertainment]  \n",
      "123370                    [Entertainment]  \n",
      "...                                   ...  \n",
      "37375                          [Politics]  \n",
      "27622   [Politics, Entertainment, Sports]  \n",
      "67006                          [Business]  \n",
      "203749                           [Sports]  \n",
      "111959                    [Entertainment]  \n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df_support_sample = df_support.sample(n=100)\n",
    "print(df_support_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    6589\n",
      "0    5237\n",
      "Name: label, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(df_support_sample['neighbor'])\n",
    "\n",
    "## getting the distribution of labels in the dev + train + test data\n",
    "# print(df_train['label'].value_counts())\n",
    "# print(df_test['label'].value_counts())\n",
    "# print(df_dev['label'].value_counts())\n",
    "\n",
    "\n",
    "print(df_train['label'].value_counts() + df_test['label'].value_counts() + df_dev['label'].value_counts())\n",
    "\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76293e17429ae2c839469a84a4692f69b1d764ad81a6a044a99c52430bb10e84"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
