{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset import Twibot20\n",
    "# import torch\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dev.json\n",
      "Loading train.json\n",
      "Loading test.json\n",
      "Loading support.json\n",
      "size of support.json: (217754, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Loading dev.json')\n",
    "df_dev=pd.read_json('./Twibot-20/dev.json')\n",
    "print('Loading train.json')\n",
    "df_train=pd.read_json('./Twibot-20/train.json')\n",
    "print('Loading test.json')\n",
    "df_test=pd.read_json('./Twibot-20/test.json')\n",
    "\n",
    "print('Loading support.json')\n",
    "df_support=pd.read_json('./Twibot-20/support.json')\n",
    "print('size of support.json:',df_support.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examineDataframes():\n",
    "    print('Examining Dataframes')\n",
    "    print('Columns:')\n",
    "    print(df_dev.columns)\n",
    "\n",
    "    print('\\n\\n')\n",
    "    print('Contents of tweet for index 1')\n",
    "    # print(df_dev.iloc[1]['tweet'])\n",
    "    # print(type(df_dev.iloc[1]['tweet']))\n",
    "    \n",
    "    print('Number of nodes in train.json:')\n",
    "    print(len(df_train))\n",
    "   \n",
    "    print('Number of nodes in test.json:')\n",
    "    print(len(df_test))\n",
    "\n",
    "    print('Number of nodes in dev.json:')\n",
    "    print(len(df_dev))\n",
    "    \n",
    "    \n",
    "    pass\n",
    "\n",
    "def langDetectDataset():\n",
    "    # print('Loading train.json')\n",
    "    # df_train=pd.read_json('./Twibot-20/train.json')\n",
    "    # print('Loading test.json')\n",
    "    # df_test=pd.read_json('./Twibot-20/test.json')\n",
    "    # print('Loading support.json')\n",
    "    # df_support=pd.read_json('./Twibot-20/support.json')\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "    nlp.add_pipe('language_detector', last=True)\n",
    "    \n",
    "    # df_combined = pd.concat([df_train, df_test, df_dev, df_support])\n",
    "    print('testing lang detection on test.json')\n",
    "    df_test['lang'] = df_test['tweet'].apply(lambda x: pd.Series(map(lambda y: nlp(y)._.language, x)) if x else None)\n",
    "    print(\"Language value count result for df_test:\")\n",
    "    print(df_test['lang'].value_counts())\n",
    "\n",
    "    # print('Language value count result for combined')\n",
    "    # print(df_combined['tweet'].apply(lambda x: pd.Series(x)).apply(lambda x: nlp(x)._.language).stack().value_counts())\n",
    "    # print(df_combined['lang'].value_counts())\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "def basicLangDetectExample():\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "    nlp.add_pipe('language_detector', last=True)\n",
    "    text = 'This is an english text.'\n",
    "    text2 = 'Ich bin ein deutscher Text.'\n",
    "    text3 = \"Je suis un texte en français.\"\n",
    "    doc = nlp(text)\n",
    "    doc2 = nlp(text2)\n",
    "    doc3 = nlp(text3)\n",
    "    # document level language detection. Think of it like average language of the document!\n",
    "    print(doc._.language)\n",
    "    # sentence level language detection\n",
    "    for sent in doc.sents:\n",
    "        print(sent, sent._.language)\n",
    "\n",
    "     # document level language detection. Think of it like average language of the document!\n",
    "    print(doc2._.language)\n",
    "    # sentence level language detection\n",
    "    for sent in doc2.sents:\n",
    "        print(sent, sent._.language)\n",
    "    \n",
    "     # document level language detection. Think of it like average language of the document!\n",
    "    print(doc3._.language)\n",
    "    # sentence level language detection\n",
    "    for sent in doc3.sents:\n",
    "        print(sent, sent._.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining Dataframes\n",
      "Columns:\n",
      "Index(['ID', 'profile', 'tweet', 'neighbor', 'domain', 'label'], dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "Contents of tweet for index 1\n",
      "Number of nodes in train.json:\n",
      "8278\n",
      "Number of nodes in test.json:\n",
      "1183\n",
      "Number of nodes in dev.json:\n",
      "2365\n"
     ]
    }
   ],
   "source": [
    "examineDataframes()\n",
    "# langDetectDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         ID  \\\n",
      "61142    740774942275436544   \n",
      "37904   1298629881085992960   \n",
      "49569            3364843913   \n",
      "159246             60604911   \n",
      "55363              68781502   \n",
      "...                     ...   \n",
      "185134           1827438553   \n",
      "56380             787542992   \n",
      "41790   1225242834795667456   \n",
      "49596   1191625970161201152   \n",
      "143065           2867022183   \n",
      "\n",
      "                                                  profile  \\\n",
      "61142   {'id': '740774942275436544 ', 'id_str': '74077...   \n",
      "37904   {'id': '1298629881085992961 ', 'id_str': '1298...   \n",
      "49569   {'id': '3364843913 ', 'id_str': '3364843913 ',...   \n",
      "159246  {'id': '60604911 ', 'id_str': '60604911 ', 'na...   \n",
      "55363   {'id': '68781502 ', 'id_str': '68781502 ', 'na...   \n",
      "...                                                   ...   \n",
      "185134  {'id': '1827438553 ', 'id_str': '1827438553 ',...   \n",
      "56380   {'id': '787542992 ', 'id_str': '787542992 ', '...   \n",
      "41790   {'id': '1225242834795667456 ', 'id_str': '1225...   \n",
      "49596   {'id': '1191625970161201153 ', 'id_str': '1191...   \n",
      "143065  {'id': '2867022183 ', 'id_str': '2867022183 ',...   \n",
      "\n",
      "                                                    tweet neighbor  \\\n",
      "61142   [RT @jenovie_t: Black         Lives           ...     None   \n",
      "37904                                                None     None   \n",
      "49569   [RT @CitronResearch: $NKLA back to $40 in a mo...     None   \n",
      "159246  [@DonnellyStephen Low figures again in Kildare...     None   \n",
      "55363   [#NRCS South Dakota is hiring a Civil Engineer...     None   \n",
      "...                                                   ...      ...   \n",
      "185134  [@OD_ant We love you regardless of your person...     None   \n",
      "56380   [@J_ssicaNichole Bryan Trottier!\\n, @MAGodin @...     None   \n",
      "41790   [RT @votevets: NEW – PREBUTTAL TO TRUMP RNC SP...     None   \n",
      "49596   [RT @ala18510343: السلام عليكم\\n, hiii\\n, @alw...     None   \n",
      "143065  [Casting Scholomance Inn-vitational on officia...     None   \n",
      "\n",
      "                 domain  \n",
      "61142        [Business]  \n",
      "37904        [Politics]  \n",
      "49569        [Politics]  \n",
      "159246  [Entertainment]  \n",
      "55363        [Business]  \n",
      "...                 ...  \n",
      "185134         [Sports]  \n",
      "56380        [Business]  \n",
      "41790        [Politics]  \n",
      "49596        [Politics]  \n",
      "143065  [Entertainment]  \n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df_support_sample = df_support.sample(n=100)\n",
    "print(df_support_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    6589\n",
      "0    5237\n",
      "Name: label, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(df_support_sample['neighbor'])\n",
    "\n",
    "## getting the distribution of labels in the dev + train + test data\n",
    "# print(df_train['label'].value_counts())\n",
    "# print(df_test['label'].value_counts())\n",
    "# print(df_dev['label'].value_counts())\n",
    "\n",
    "\n",
    "print(df_train['label'].value_counts() + df_test['label'].value_counts() + df_dev['label'].value_counts())\n",
    "\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76293e17429ae2c839469a84a4692f69b1d764ad81a6a044a99c52430bb10e84"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
