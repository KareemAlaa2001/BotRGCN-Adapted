{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset import Twibot20\n",
    "# import torch\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dev.json\n",
      "Loading train.json\n",
      "Loading test.json\n",
      "Loading support.json\n",
      "size of support.json: (217754, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Loading dev.json')\n",
    "df_dev=pd.read_json('./Twibot-20/dev.json')\n",
    "print('Loading train.json')\n",
    "df_train=pd.read_json('./Twibot-20/train.json')\n",
    "print('Loading test.json')\n",
    "df_test=pd.read_json('./Twibot-20/test.json')\n",
    "\n",
    "print('Loading support.json')\n",
    "df_support=pd.read_json('./Twibot-20/support.json')\n",
    "print('size of support.json:',df_support.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examineDataframes():\n",
    "    print('Examining Dataframes')\n",
    "    print('Columns:')\n",
    "    print(df_dev.columns)\n",
    "\n",
    "    print('\\n\\n')\n",
    "    print('Contents of tweet for index 1')\n",
    "    # print(df_dev.iloc[1]['tweet'])\n",
    "    # print(type(df_dev.iloc[1]['tweet']))\n",
    "    \n",
    "    print('Number of nodes in train.json:')\n",
    "    print(len(df_train))\n",
    "   \n",
    "    print('Number of nodes in test.json:')\n",
    "    print(len(df_test))\n",
    "\n",
    "    print('Number of nodes in dev.json:')\n",
    "    print(len(df_dev))\n",
    "    \n",
    "    \n",
    "    pass\n",
    "\n",
    "def langDetectDataset():\n",
    "    # print('Loading train.json')\n",
    "    # df_train=pd.read_json('./Twibot-20/train.json')\n",
    "    # print('Loading test.json')\n",
    "    # df_test=pd.read_json('./Twibot-20/test.json')\n",
    "    # print('Loading support.json')\n",
    "    # df_support=pd.read_json('./Twibot-20/support.json')\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "    nlp.add_pipe('language_detector', last=True)\n",
    "    \n",
    "    # df_combined = pd.concat([df_train, df_test, df_dev, df_support])\n",
    "    print('testing lang detection on test.json')\n",
    "    df_test['lang'] = df_test['tweet'].apply(lambda x: pd.Series(map(lambda y: nlp(y)._.language, x)) if x else None)\n",
    "    print(\"Language value count result for df_test:\")\n",
    "    print(df_test['lang'].value_counts())\n",
    "\n",
    "    # print('Language value count result for combined')\n",
    "    # print(df_combined['tweet'].apply(lambda x: pd.Series(x)).apply(lambda x: nlp(x)._.language).stack().value_counts())\n",
    "    # print(df_combined['lang'].value_counts())\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "def basicLangDetectExample():\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "    nlp.add_pipe('language_detector', last=True)\n",
    "    text = 'This is an english text.'\n",
    "    text2 = 'Ich bin ein deutscher Text.'\n",
    "    text3 = \"Je suis un texte en franÃ§ais.\"\n",
    "    doc = nlp(text)\n",
    "    doc2 = nlp(text2)\n",
    "    doc3 = nlp(text3)\n",
    "    # document level language detection. Think of it like average language of the document!\n",
    "    print(doc._.language)\n",
    "    # sentence level language detection\n",
    "    for sent in doc.sents:\n",
    "        print(sent, sent._.language)\n",
    "\n",
    "     # document level language detection. Think of it like average language of the document!\n",
    "    print(doc2._.language)\n",
    "    # sentence level language detection\n",
    "    for sent in doc2.sents:\n",
    "        print(sent, sent._.language)\n",
    "    \n",
    "     # document level language detection. Think of it like average language of the document!\n",
    "    print(doc3._.language)\n",
    "    # sentence level language detection\n",
    "    for sent in doc3.sents:\n",
    "        print(sent, sent._.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining Dataframes\n",
      "Columns:\n",
      "Index(['ID', 'profile', 'tweet', 'neighbor', 'domain', 'label'], dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "Contents of tweet for index 1\n",
      "Number of nodes in train.json:\n",
      "8278\n",
      "Number of nodes in test.json:\n",
      "1183\n",
      "Number of nodes in dev.json:\n",
      "2365\n"
     ]
    }
   ],
   "source": [
    "examineDataframes()\n",
    "# langDetectDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         ID  \\\n",
      "95433   1263502211964796928   \n",
      "22267                813365   \n",
      "60237    829087663764414464   \n",
      "100919  1261723931855159296   \n",
      "137419           1416051025   \n",
      "...                     ...   \n",
      "204588             71847080   \n",
      "133961           3313102861   \n",
      "192037  1281959881872945152   \n",
      "65144   1117812838922379264   \n",
      "115990             35404273   \n",
      "\n",
      "                                                  profile  \\\n",
      "95433   {'id': '1263502211964796928 ', 'id_str': '1263...   \n",
      "22267   {'id': '813365 ', 'id_str': '813365 ', 'name':...   \n",
      "60237   {'id': '829087663764414464 ', 'id_str': '82908...   \n",
      "100919  {'id': '1261723931855159296 ', 'id_str': '1261...   \n",
      "137419  {'id': '1416051025 ', 'id_str': '1416051025 ',...   \n",
      "...                                                   ...   \n",
      "204588  {'id': '71847080 ', 'id_str': '71847080 ', 'na...   \n",
      "133961  {'id': '3313102861 ', 'id_str': '3313102861 ',...   \n",
      "192037  {'id': '1281959881872945154 ', 'id_str': '1281...   \n",
      "65144   {'id': '1117812838922379266 ', 'id_str': '1117...   \n",
      "115990  {'id': '35404273 ', 'id_str': '35404273 ', 'na...   \n",
      "\n",
      "                                                    tweet neighbor  \\\n",
      "95433                                                None     None   \n",
      "22267   [@DuardaumTv @OGTENN @MSFS_Support If you're s...     None   \n",
      "60237   [RT @HockeyByDesign: Absolutely incredible mas...     None   \n",
      "100919  [RT @donwinslow: Can we dig Herman up and have...     None   \n",
      "137419  [RT @cinemawaleghosh: Her famous fringe haircu...     None   \n",
      "...                                                   ...      ...   \n",
      "204588  [RT @someotheralex: Aren't you the Chair of th...     None   \n",
      "133961                                               None     None   \n",
      "192037                                               None     None   \n",
      "65144   [@usajedi Silent about human trafficking and t...     None   \n",
      "115990  [RT @WeavRun: ðŸš¨Mind Over MilesðŸš¨\\nBeat Your Bes...     None   \n",
      "\n",
      "                 domain  \n",
      "95433        [Business]  \n",
      "22267        [Politics]  \n",
      "60237        [Business]  \n",
      "100919       [Business]  \n",
      "137419  [Entertainment]  \n",
      "...                 ...  \n",
      "204588         [Sports]  \n",
      "133961  [Entertainment]  \n",
      "192037         [Sports]  \n",
      "65144        [Business]  \n",
      "115990  [Entertainment]  \n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df_support_sample = df_support.sample(n=100)\n",
    "print(df_support_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    6589\n",
      "0    5237\n",
      "Name: label, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(df_support_sample['neighbor'])\n",
    "\n",
    "## getting the distribution of labels in the dev + train + test data\n",
    "# print(df_train['label'].value_counts())\n",
    "# print(df_test['label'].value_counts())\n",
    "# print(df_dev['label'].value_counts())\n",
    "\n",
    "\n",
    "print(df_train['label'].value_counts() + df_test['label'].value_counts() + df_dev['label'].value_counts())\n",
    "\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76293e17429ae2c839469a84a4692f69b1d764ad81a6a044a99c52430bb10e84"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
