importing the dataset...
Loading train.json
Loading test.json
Small dataset version, not loading support.json
Loading dev.json
  0%|                                                                                                          | 0/100 [00:00<?, ?it/s]
Finished
Loading labels...   Finished
Loading user description embeddings
Finished
Running tweet embedding
Finished
Processing feature3...   Finished
Processing feature4...   Finished
Building graph   Finished
beginning training...
<class 'dict'>
dict_keys(['user', 'tweet'])
<class 'torch.Tensor'>

  1%|â–‰                                                                                                 | 1/100 [00:25<41:41, 25.27s/it]
Traceback (most recent call last):
  File "trainTestHetero.py", line 116, in <module>
    train(epoch)
  File "trainTestHetero.py", line 68, in train
    output = model(dataset)
  File "/usr/local/anaconda3/envs/mlp/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/kareem/EdiStuff/Dissertation/Code/BotRGCN-Adapted/model.py", line 98, in forward
    t=self.linear_relu_tweet(data['tweet'].x.float())
  File "/usr/local/anaconda3/envs/mlp/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/anaconda3/envs/mlp/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/usr/local/anaconda3/envs/mlp/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/anaconda3/envs/mlp/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/usr/local/anaconda3/envs/mlp/lib/python3.7/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1999788x96 and 100x96)